{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SED_AGE_ED127_22.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/DL_ED127_2022/blob/main/hands-on/CNNs/SED_AGE_ED127_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jatIhJ9KJQV"
      },
      "source": [
        "#Galaxy Star Formation History with deep learning\n",
        "\n",
        "The goal of this notebook is to develop a deep learning model which takes a galaxy Spectral Energy Distribution (SED) and estimates some parameters of the history of star formation (SFH), i.e. the star formation rate as a function of comsic time. \n",
        "\n",
        "We will use to that purpose data from the hydrodinamic cosmological simulation [EAGLE.](https://eagle.strw.leidenuniv.nl/), for which we can extract the Star Formation Histories. We then take galaxies at z=0 and generate mock SEDs based on their stellar populations. \n",
        "\n",
        "The goal is therefore to establish a mapping between the SED and the SFH. The SFH is parametrized with some moments, t_xx measuring the time at which xx of the stellar mass of the galaxy has been formed. That way the problem is converted into a regression. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpv-XO8Sxgvi"
      },
      "source": [
        "#%pylab inline\n",
        "!pip install -q tfds-nightly\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "data_dir ='/content/drive/MyDrive/ED127_2022/EAGLE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PqZCKlVSMJ"
      },
      "source": [
        "## Mount Drive\n",
        "\n",
        "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
        "\n",
        "*   Go to your drive \n",
        "*   Find shared folder (\"Shared with me\" link)\n",
        "*   Right click it\n",
        "*   \"Add Shortcut to Drive\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Io82GXlCNdF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Tensorflow dataset"
      ],
      "metadata": {
        "id": "j40llCHG6R4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, \"/content/drive/MyDrive/ED127_2022/EAGLE\")"
      ],
      "metadata": {
        "id": "P_oE33270Cem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eagle"
      ],
      "metadata": {
        "id": "2UHrMR8Pt6I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell loads the dataset. We use here Tensorflow Dataset which is a convenient way of generating the data for training. There are predefined datasets. In our case, the code to build the dataset is in the file /content/drive/MyDrive/ED127_2022/EAGLE/eagle.py"
      ],
      "metadata": {
        "id": "--jb2jcsobMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dset = tfds.load('eagle', split='train', data_dir=data_dir)"
      ],
      "metadata": {
        "id": "g_06skALuxgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizes some trainng data"
      ],
      "metadata": {
        "id": "fyzZndMW3oqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(\"Train\",len(dset))\n",
        "\n",
        "fig, axs = plt.subplots(1, 1)\n",
        "for example in dset.take(1):\n",
        "    #print((example['wl_sort'])[example['inds_valid']])\n",
        "    time_vec = example['time']\n",
        "    inds_valid = example['inds_valid']\n",
        "    axs.scatter((example['wl_sort'])[example['inds_valid']],np.log10(example['sed']))\n",
        "    axs.set_xscale('log')\n",
        "    plt.xlabel(\"Wavelength\",fontsize=20)\n",
        "    plt.ylabel(\"Flux\",fontsize=20)\n",
        "\n",
        "fig, axs = plt.subplots(1, 1)\n",
        "for example in dset.take(1):\n",
        "    #print((example['wl_sort'])[example['inds_valid']])\n",
        "    time_vec = example['time']\n",
        "    axs.scatter(time_vec,example[\"Mstar\"])\n",
        "    axs.set_yscale('log')\n",
        "    plt.xlabel(\"Normalized Cosmic Time\",fontsize=20)\n",
        "    plt.ylabel(\"Stellar Mass\",fontsize=20)\n",
        "    #mass_history_summaries = find_summaries(np.flip(example['Mstar']),\n",
        "                                                #np.flip(example['time']))\n",
        "    #print(mass_history_summaries)\n",
        "    for m in example[\"mass_quantiles\"]:\n",
        "      plt.axvline(x=m,color='red')  "
      ],
      "metadata": {
        "id": "15v8QvK5iTLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "xnBrXZPn6cj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates the functions for training. "
      ],
      "metadata": {
        "id": "V7ef7jZa3vZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function outputs the t50 value (time at which half of the mass has been formed)\n",
        "# can be modified\n",
        "def preprocessing(example):\n",
        "    sed = -1*tf.experimental.numpy.log10(example['sed'])/2\n",
        "    t50 = (example['mass_quantiles'])[:,4]/100\n",
        "    return sed, t50\n",
        "\n",
        "def input_fn(mode='train', batch_size=64, \n",
        "             dataset_name='tng100', data_dir=None,\n",
        "             include_mass=True, arctan=True):\n",
        "    \"\"\"\n",
        "    mode: 'train' or 'test'\n",
        "    \"\"\"\n",
        "    keys = ['sed','mass_quantiles']\n",
        "    if mode == 'train':\n",
        "        dataset = tfds.load(dataset_name, split='train[:80%]', data_dir=data_dir)\n",
        "        dataset = dataset.map(lambda x: {k:x[k] for k in keys})\n",
        "        dataset = dataset.repeat()\n",
        "        dataset = dataset.shuffle(10000)\n",
        "    elif mode=='val':\n",
        "        dataset = tfds.load(dataset_name, split='train[80%:90%]', data_dir=data_dir)\n",
        "        dataset = dataset.map(lambda x: {k:x[k] for k in keys}) \n",
        "    else:\n",
        "        dataset = tfds.load(dataset_name, split='train[90%:]', data_dir=data_dir)\n",
        "        dataset = dataset.map(lambda x: {k:x[k] for k in keys})\n",
        "        \n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    dataset = dataset.map(preprocessing)\n",
        "    dataset = dataset.prefetch(-1)       # fetch next batches while training current one (-1 for autotune)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "unO4MvBBpt4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates the datasets for training, validation and test"
      ],
      "metadata": {
        "id": "Fl6KCvxf5ryd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "dtrain = input_fn(mode='train', batch_size=batch_size, dataset_name='eagle',data_dir=data_dir)\n",
        "dval = input_fn(mode='val', batch_size=batch_size, dataset_name='eagle',data_dir=data_dir)\n",
        "dtest = input_fn(mode='test', batch_size=batch_size, dataset_name='eagle',data_dir=data_dir)"
      ],
      "metadata": {
        "id": "o4Q3HOhArqtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates the model"
      ],
      "metadata": {
        "id": "ML-byC1p6F5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "num_components = 1\n",
        "event_shape = [1]\n",
        "\n",
        "params_size = tfp.layers.MixtureSameFamily.params_size(\n",
        "    num_components,\n",
        "    component_params_size=tfp.layers.IndependentNormal.params_size(event_shape))\n",
        "\n",
        "sed_net = tf.keras.Sequential([\n",
        "        tfkl.Input(shape=(125, 1)),\n",
        "        \n",
        "        #ADD 1D convolutions here\n",
        "\n",
        "        tfkl.Flatten(),\n",
        "        tfkl.Dense(128, activation='relu'),\n",
        "        tfkl.Dropout(0.4),\n",
        "        tfkl.Dense(8, activation='softplus'),\n",
        "        tfkl.Dense(units=params_size, activation=None),\n",
        "        tfp.layers.MixtureSameFamily(num_components, tfp.layers.IndependentNormal(event_shape))\n",
        "])\n",
        "\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "sed_net.compile(loss=negloglik, optimizer=opt)"
      ],
      "metadata": {
        "id": "oXpdB63krxpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sed_net.summary()"
      ],
      "metadata": {
        "id": "x3ok9bJj4PTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "Pm25Pxgq6KJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = sed_net.fit(dtrain, \n",
        "                     epochs=epochs,\n",
        "                     steps_per_epoch=1000,validation_data=dval)"
      ],
      "metadata": {
        "id": "VbOC_7sisy0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of results"
      ],
      "metadata": {
        "id": "uBlI5_Ia6f3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets the mean and standard deviations of the posteriors estimated by the neural network"
      ],
      "metadata": {
        "id": "m4KUQL0-6ngz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_est_avg = np.concatenate([sed_net(batch[0]).mean() for batch in dtest])\n",
        "time_est_std = np.concatenate([sed_net(batch[0]).stddev() for batch in dtest])"
      ],
      "metadata": {
        "id": "cHr_fSROuEXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets the ground truth"
      ],
      "metadata": {
        "id": "pDfttA7e7E2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = []\n",
        "for element in dtest.as_numpy_iterator(): \n",
        "  total.append(element[1])\n",
        "\n",
        "time_true = np.concatenate(total)  "
      ],
      "metadata": {
        "id": "8CMU5IsQwcFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plots the results"
      ],
      "metadata": {
        "id": "pXxdgfVM7Hja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(time_true,time_est_avg,yerr=time_est_std,color='black',fmt=\"none\")\n",
        "plt.scatter(time_true,time_est_avg,color='red',s=2)\n",
        "plt.plot(np.linspace(0,1,100),np.linspace(0,1,100),color='red')"
      ],
      "metadata": {
        "id": "XxbjEp5sxpYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercices"
      ],
      "metadata": {
        "id": "K3rDZ8L99WyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 1: \n",
        "Create a model that predicts several times simultaneously."
      ],
      "metadata": {
        "id": "aLUm3Bvi9ZkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 2:\n",
        "\n",
        "Create a model that uses only the optical/NIR part of the SED as input, i.e. wl < 20.000 A\n"
      ],
      "metadata": {
        "id": "M05TNkPi-dVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 3\n",
        "\n",
        "Add 10% error to the SED and estimate the impact on the results"
      ],
      "metadata": {
        "id": "xTDXM34b_-n-"
      }
    }
  ]
}